{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:46:44.400627Z",
     "start_time": "2022-11-04T12:46:44.385624Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:46:44.901424Z",
     "start_time": "2022-11-04T12:46:44.882515Z"
    }
   },
   "outputs": [],
   "source": [
    "class Q_agent(object):\n",
    "    \"\"\"Basic Q-learning with epsilon-greedy policy.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions,\n",
    "                 greedy_eps = 1, \n",
    "                 exploration_decay_rate = 0.001, \n",
    "                 discount_rate = 0.99,\n",
    "                 lr = 0.1,\n",
    "                 min_exploration_rate = 0.001,\n",
    "                 max_exploration_rate = 1):\n",
    "        \n",
    "        \n",
    "        self.greedy_eps0 = greedy_eps\n",
    "        self.greedy_eps = self.greedy_eps0\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.lr = lr\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        \n",
    "        # initiliaze all Q-table values to 0\n",
    "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "    \n",
    "    \n",
    "    def reset_Q(self, n_states, n_actions):\n",
    "        self.Q = np.zeros(self.n_states, self.n_actions)\n",
    "    \n",
    "    \n",
    "    def make_action(self, observation, exploit_only=False):\n",
    "        r = random.uniform(0, 1)\n",
    "        \n",
    "        if r > self.greedy_eps or (exploit_only):\n",
    "            # exploit \n",
    "            action = np.argmax(self.Q[observation, :])\n",
    "            \n",
    "        else:\n",
    "            # explore (take a random action)\n",
    "            action = np.random.randint(0,self.n_actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def update_table(self, old_state, new_state, reward, action, episode, t=0):\n",
    "        self.Q[old_state,action] = (1-self.lr)*self.Q[old_state,action] + self.lr*(reward + self.discount_rate * np.max(self.Q[new_state, :]) )\n",
    "        \n",
    "        # update greedy eps\n",
    "        self.greedy_eps = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate * episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:46:46.201795Z",
     "start_time": "2022-11-04T12:46:46.178842Z"
    }
   },
   "outputs": [],
   "source": [
    "def frozen_lake_train(n_episodes, n_timesteps=int(1e10), greedy_eps = 1, \n",
    "                      exploration_decay_rate = 0.001, discount_rate = 0.99, lr = 0.1,\n",
    "                      min_exploration_rate = 0.001, max_exploration_rate = 1,\n",
    "                      desc=None, map_name=\"4x4\", is_slippery=True):\n",
    "    \n",
    "    #create environment frozen lake\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=desc, map_name=map_name, is_slippery=is_slippery, render_mode=\"rgb_array_list\")\n",
    "    \n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    #create agent\n",
    "    agent = Q_agent(n_states,n_actions,greedy_eps, \n",
    "                exploration_decay_rate, \n",
    "                discount_rate,\n",
    "                lr,\n",
    "                min_exploration_rate,\n",
    "                max_exploration_rate)\n",
    "    reward=None\n",
    "    \n",
    "    \n",
    "    #TODO create total reward\n",
    "    reward_list = []\n",
    "    \n",
    "    #training of agent\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        for timestep in range(n_timesteps):\n",
    "            \n",
    "            #plt.imshow(env.render());\n",
    "            #print(observation)\n",
    "            \n",
    "            if done:\n",
    "                #print(\"Episode finished after {} timesteps\".format(timestep + 1))\n",
    "                break\n",
    "            \n",
    "            action = agent.make_action(state, exploit_only=False)\n",
    "            #print(agent.greedy_eps)\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "            agent.update_table(state, observation, reward, action, episode, timestep)\n",
    "            state = observation\n",
    "            \n",
    "            # sum up the number of rewards after n episodes\n",
    "            total_reward += reward\n",
    "           \n",
    "        \n",
    "        reward_list.append(total_reward)\n",
    "    #print(np.cumsum(reward_list)/(np.arange(n_episodes) + 1))\n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.plot((np.arange(n_episodes) + 1), np.cumsum(reward_list)/(np.arange(n_episodes) + 1))\n",
    "    plt.show()\n",
    "    env.close()\n",
    "    return (reward_list, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:46:47.725489Z",
     "start_time": "2022-11-04T12:46:47.699524Z"
    }
   },
   "outputs": [],
   "source": [
    "def frozen_lake_play(env, agent, n_episodes, n_timesteps=int(1e10),\n",
    "                      desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"human\"):\n",
    "    \n",
    "    #create environment frozen lake\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=desc, map_name=map_name, is_slippery=is_slippery, render_mode=render_mode)\n",
    "    \n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    reward=None\n",
    "    \n",
    "    #TODO create total reward\n",
    "    reward_list = []\n",
    "    \n",
    "    # playing\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        for timestep in range(n_timesteps):\n",
    "            \n",
    "            env.render()\n",
    "            #print(observation)\n",
    "           \n",
    "            if done:\n",
    "                #print(\"Episode finished after {} timesteps\".format(timestep + 1))\n",
    "                break\n",
    "            \n",
    "            action = agent.make_action(state, exploit_only=True)\n",
    "            #print(agent.greedy_eps)\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "            state = observation\n",
    "            \n",
    "            # sum up the number of rewards after n episodes\n",
    "            total_reward += reward\n",
    "        \n",
    "        reward_list.append(total_reward)\n",
    "    #print(np.cumsum(reward_list)/(np.arange(n_episodes) + 1))\n",
    "    env.close()\n",
    "    return (reward_list, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T13:45:44.137336Z",
     "start_time": "2022-11-04T13:45:37.027014Z"
    }
   },
   "outputs": [],
   "source": [
    "desc=None\n",
    "map_name=\"4x4\"#\"4x4\"\"8x8\"\n",
    "is_slippery=False\n",
    "\n",
    "rewards, agent, env = frozen_lake_train(10000, n_timesteps=int(1e10), greedy_eps = 1, \n",
    "                                 exploration_decay_rate = 0.001, discount_rate = 0.99, lr = 0.1,\n",
    "                                 min_exploration_rate = 0.001, max_exploration_rate = 1,\n",
    "                                 desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T13:46:04.345503Z",
     "start_time": "2022-11-04T13:45:49.713951Z"
    }
   },
   "outputs": [],
   "source": [
    "rewards, agent, env = frozen_lake_play(env, agent, 4,\n",
    "                      desc=desc, map_name=map_name, is_slippery=is_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-03T17:18:36.741242Z",
     "start_time": "2022-11-03T17:18:36.722364Z"
    }
   },
   "outputs": [],
   "source": [
    "#class deprecated_Q_agent_oneshot(object):\n",
    "#    \n",
    "#    def __init__(self, n_states, n_actions,\n",
    "#                 greedy_eps = 1, \n",
    "#                exploration_decay_rate = 0.001, \n",
    "#                discount_rate = 0.99,\n",
    "#                lr = 0.1,\n",
    "#                min_exploration_rate = 0.001,\n",
    "#                max_exploration_rate = 1):\n",
    "#        \n",
    "#        \n",
    "#        self.greedy_eps0 = greedy_eps\n",
    "#        self.greedy_eps = self.greedy_eps0\n",
    "#        self.n_states = n_states\n",
    "#        self.n_actions = n_actions\n",
    "#        \n",
    "#        self.exploration_decay_rate = exploration_decay_rate\n",
    "#        self.discount_rate = discount_rate\n",
    "#        self.lr = lr\n",
    "#        self.min_exploration_rate = min_exploration_rate\n",
    "#        self.max_exploration_rate = max_exploration_rate\n",
    "#        \n",
    "#        self.old_state=None\n",
    "#        \n",
    "#        # initiliaze all q-table values to 0\n",
    "#        self.Q = np.zeros((self.n_states, self.n_actions))\n",
    "#        #\n",
    "#        \n",
    "#        \n",
    "#    def reset_Q(self, n_states, n_actions):\n",
    "#        self.Q = np.zeros(self.n_states, self.n_actions)\n",
    "#    \n",
    "#    def explore_step(self, observation, reward, t=0, episode=0):\n",
    "#        r = random.uniform(0, 1)\n",
    "#        \n",
    "#        if r > self.greedy_eps:\n",
    "#            # exploit \n",
    "#            action = np.argmax(self.Q[observation, :])\n",
    "#            \n",
    "#        else:\n",
    "#            # explore (take a random action)\n",
    "#            action = np.random.randint(0,self.n_actions)\n",
    "#        \n",
    "#        if t>0:\n",
    "#            self.Q[self.old_state,action] = (1-self.lr)*self.Q[self.old_state,action] + self.lr*(reward + self.discount_rate * np.max(self.Q[observation, :]) )\n",
    "#\n",
    "#            \n",
    "#        # update greedy eps\n",
    "#        self.greedy_eps = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate * episode)\n",
    "#\n",
    "#        self.old_state = observation\n",
    "#        return action\n",
    "#        \n",
    "#    \n",
    "#    def action(self, observation, reward):\n",
    "#        \n",
    "#        pass\n",
    "#    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "350.747px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677.587px",
    "left": "1740.2px",
    "right": "20px",
    "top": "147.972px",
    "width": "338.281px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
